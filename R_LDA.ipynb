{"cells":[{"cell_type":"markdown","source":["## CIS5560: PySpark LDA Text Analysis for \"R\" answers and Tags analysis\n\nTested in Python 2 with Spark 2.1"],"metadata":{}},{"cell_type":"markdown","source":["## Text Analysis using Latent Dirichlet Allocation (LDA)\n\n### Importing Spark SQL and Spark ML Libraries\nFirst, importing the libraries needed:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.linalg import Vectors, SparseVector\nfrom pyspark.ml.clustering import LDA, BisectingKMeans\nfrom pyspark.sql.functions import monotonically_increasing_id\n\nimport re"],"metadata":{"scrolled":false},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["### Load Source Data\nNow load the Answers data into a DataFrame. This data consists of Information about the Questions, Tags, Date, Is Accepted, and comments from user."],"metadata":{}},{"cell_type":"code","source":["IS_DSX = False\n\n# IBM DSX: pixiedust for visualization\nif IS_DSX:\n  from pixiedust.display import *"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Installing pixiedust for visualization"],"metadata":{}},{"cell_type":"code","source":["!pip install pixiedust"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["#### Create a table: R__Answers_LDA with R_Answers.csv_ file\n##### all colums are of Integer except (IsAcceptedAnswer: String, Body: String, Tag: String)"],"metadata":{}},{"cell_type":"code","source":["\nif IS_DSX == False:\n  df_data_answers = sqlContext.sql(\"select OwnerUserId,Year,Month,Day,CreationDate,ParentId,Score,IsAcceptedAnswer,Body from r_answers_lda_csv\")\n  df_data_ans_clean = df_data_answers.dropna()\n  df_data_ans_clean.show(5)\n"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["if IS_DSX == False:\n  df_data_tags = sqlContext.sql(\"select * from r_tags_csv\")\n  df_data_tags_clean = df_data_tags.dropna()\n  df_data_tags_clean.show(5)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["joined = df_data_ans_clean.join(df_data_tags_clean,df_data_ans_clean.ParentId == df_data_tags_clean.Id )\njoined.show(5)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["rawdata = joined\n\n#Show rawdata (as DataFrame)\nrawdata.show(5)"],"metadata":{"scrolled":false},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Print data types\nfor type in rawdata.dtypes:\n    print type\n"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Text Pre-processing:\n###   \n1. Removing common words (with stoplist)\n1. Handling punctuation\n1. lowcase/upcase\n1. Stemming\n1. Part-of-Speech Tagging (nouns, verbs, adj, etc.)"],"metadata":{}},{"cell_type":"code","source":["def cleanup_text(record):\n    text  = record[8]\n    uid   = record[9]\n    words = text.split()\n    \n    # Default list of Stopwords\n    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n    u'can', 'cant', 'come', u'could', 'couldnt', \n    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n    u'each', \n    u'few', 'finally', u'for', u'from', u'further', \n    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n    u'just', \n    u'll', \n    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n    u'no', u'nor', u'not', u'now', \n    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n    u'r', u're', \n    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n    u'under', u'until', u'up', \n    u'very', \n    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n    \n    # Custom List of Stopwords - Add your own here\n    stopwords_custom = ['<p>']\n    stopwords = stopwords_core + stopwords_custom\n    stopwords = [word.lower() for word in stopwords]    \n    \n    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in words]                                       \n    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]    \n    return text_out\n\n"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Cleaning Text"],"metadata":{}},{"cell_type":"code","source":["udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\nclean_text = rawdata.withColumn(\"words\", udf_cleantext(struct([rawdata[x] for x in rawdata.columns])))\nclean_text.show(5)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Generate TFIDF and Vectorize it"],"metadata":{}},{"cell_type":"code","source":["# Term Frequency Vectorization  - Option 1 (Using hashingTF): \n'''hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(clean_text)\n'''\n# Term Frequency Vectorization  - Option 2 (CountVectorizer)    : \ncv = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\", vocabSize = 1000)\ncvmodel = cv.fit(clean_text)\nfeaturizedData = cvmodel.transform(clean_text)\n\nvocab = cvmodel.vocabulary\nvocab_broadcast = sc.broadcast(vocab)\n"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Term Frequency (TF)\n### Inverse Document Frequency (IDF)"],"metadata":{}},{"cell_type":"code","source":["idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["## LDA Clustering \n### Generate 25 Data-Driven Topics:"],"metadata":{}},{"cell_type":"code","source":["# Generate 25 Data-Driven Topics:\nlda = LDA(k=25, seed=123, optimizer=\"em\", featuresCol=\"features\")\n\nldamodel = lda.fit(rescaledData)\n\n#model.isDistributed()\n#model.vocabSize()\n\nldatopics = ldamodel.describeTopics()\n#ldatopics.show(25)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["ldatopics.show(25)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### LDA Clustering - Find Data-driven Topics"],"metadata":{}},{"cell_type":"code","source":["\n\ndef map_termID_to_Word(termIndices):\n    words = []\n    for termID in termIndices:\n        words.append(vocab_broadcast.value[termID])\n    \n    return words\n\nudf_map_termID_to_Word = udf(map_termID_to_Word , ArrayType(StringType()))\nldatopics_mapped = ldatopics.withColumn(\"topic_desc\", udf_map_termID_to_Word(ldatopics.termIndices))\nldatopics_mapped.select(ldatopics_mapped.topic, ldatopics_mapped.topic_desc).show(50,False)\n"],"metadata":{"scrolled":false},"outputs":[],"execution_count":26},{"cell_type":"code","source":["ldaResults = ldamodel.transform(rescaledData)\n\nldaResults.select('OwnerUserId','Year','Month','Day','CreationDate','Score','IsAcceptedAnswer','Tag','words','features','topicDistribution').show()"],"metadata":{"scrolled":false},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### Breakout LDA Topics for Modeling and Reporting"],"metadata":{}},{"cell_type":"code","source":["def breakout_array(index_number, record):\n    vectorlist = record.tolist()\n    return vectorlist[index_number]\n\nudf_breakout_array = udf(breakout_array, FloatType())\n\n# Extract document weights for Topics 12 and 20\nenrichedData = ldaResults                                                                   \\\n        .withColumn(\"Topic_12\", udf_breakout_array(lit(12), ldaResults.topicDistribution))  \\\n        .withColumn(\"topic_20\", udf_breakout_array(lit(20), ldaResults.topicDistribution))            \n\nenrichedData.select('OwnerUserId','Year','Month','Day','CreationDate','Score','IsAcceptedAnswer','Tag','words','features','topicDistribution','Topic_12','Topic_20').show()\n\nenrichedData.agg(max(\"Topic_12\")).show()"],"metadata":{"scrolled":false},"outputs":[],"execution_count":29},{"cell_type":"code","source":["enrichedData.createOrReplaceTempView(\"enrichedData\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":["topics = enrichedData.select('OwnerUserId','Year','Month','Day', 'CreationDate', 'Score', 'Score','IsAcceptedAnswer','Tag','Topic_12','Topic_20').sort(desc(\"CreationDate\"))"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["topics.show(5)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["## Line Chart of Tags, IsAcceptedAnswer, and Topics."],"metadata":{}},{"cell_type":"code","source":["# pixiedust needed for IBM DSX\ndisplay(topics)"],"metadata":{"pixiedust":{"displayParams":{"keyFields":"date","timeseries":"true","valueFields":"rating","stretch":"false","charttype":"grouped","chartsize":"98","mpld3":"false","tableFields":"airline,date,rating","rendererId":"matplotlib","handlerId":"barChart","clusterby":"airline","orientation":"vertical","title":"5 Airlines Average Rating"}}},"outputs":[],"execution_count":34},{"cell_type":"code","source":["#display(topics)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["# Only for Databricks\n'''\n%sql\nSELECT id, airline, date, year_month, rating, topic_12, topic_20 FROM enrichedData where airline = \"${item=Delta Air Lines,Delta Air Lines|US Airways|Southwest Airlines|American Airlines|United Airlines}\" order by date\n'''"],"metadata":{},"outputs":[],"execution_count":36}],"metadata":{"kernelspec":{"display_name":"Python 2 with Spark 2.1","language":"python","name":"python2-spark21"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.14","nbconvert_exporter":"python","file_extension":".py"},"name":"Python_Text_Analysis IBM copy 1","notebookId":2496269848372493},"nbformat":4,"nbformat_minor":0}
